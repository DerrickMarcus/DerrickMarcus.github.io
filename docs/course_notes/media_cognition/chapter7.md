# 第7讲 Transformer 与大模型

## 7.1 Sequence Modeling

RNN 和 Transformer 进行序列建模(Sequence Modeling)的优缺点对比：

> Generated by ChatGPT.

RNN

- 优点：
    1. 线性时间复杂度：单层每步 $O(n)$ ， $n$ 为隐藏维度，对长序列空间占用仍然是 $O(n)$ ；计算上更易控。
    2. 参数/显存较省：只需保存一个隐藏状态，显存随层数线性增长，适合显存受限环境或嵌入式设备。
    3. 在线/流式友好：按时间步逐帧输出，可边接收边推理，延迟低，适合语音识别、工业监控等实时任务。
    4. 短序列推理快：在小模型场景中一次前向仅做几何级矩阵乘加，没有全局注意力矩阵构造，单步延迟更低。
- 缺点
    1. 无法并行：时间维度串行依赖，GPU/TPU 计算利用率低，训练速度随序列长度线性变慢。
    2. 梯度消失/爆炸：长序列信息衰减，需 LSTM/GRU 或技巧（梯度裁剪、正则）仍难完全解决远程依赖。
    3. 难以大规模扩展：深/宽网络堆叠会加重串行瓶颈；要提升效果往往靠更复杂门控或外部注意力，工程代价高。
    4. 上下文窗口有限：理论上可跨步，但实践中有效记忆长度通常几百 token 以内，长文本表现受限。

Transformer

- 优点：
    1. 高度并行：自注意力对整个序列一次性计算，充分利用 GPU 张量核心；大批数据可显著加速训练。
    2. 长程依赖建模强：每个 token 直接与任意位置交互，信息传播路径为 $O(1)$ ，捕捉全局模式能力优异。
    3. 可扩展性好：增宽或加层通常带来稳定性能提升；与分布式数据并行/模型并行天然适配。
    4. 表示通用性：统一的注意力框架易于跨模态（文本、图像、音频）迁移；位置编码灵活处理变长输入。
- 缺点
    1. 时间与显存 $O(L^2)$ ：标准自注意力需要 $L\times L$ 矩阵（L 为序列长度）；长序列（>4k）显存和计算迅速爆炸。
    2. 推理延迟随长度平方增长：在超长文本或实时场景，单句延迟明显高于 RNN；需缓存 KV 或采用稀疏/线性注意力优化。
    3. 部署成本高：大模型参数量和计算量庞大，对边端设备或低功耗场景不友好；量化/蒸馏复杂度高。
    4. 固有批处理依赖：完全并行的优势建立在足够大批尺寸上，小批/流式推理需增量注意力或交错重算，工程实现更复杂。

## 7.2 Self-attention

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

### 7.2.1 Self-attention

![202506132140774](https://cdn.jsdelivr.net/gh/DerrickMarcus/picgo-image/images/202506132140774.png)

Transformer 处理文本数据的第一步是将 token 转换为 embedding 嵌入向量，使之称为可计算的数值。

假设序列的长度为 $N$ ，相当于 $N$ 个输入向量 $\boldsymbol{x}_i$ 组成矩阵 $\boldsymbol{X}\in \mathbb{R}^{N\times d}$ ，其中第 $i$ 行为 $\boldsymbol{x}_i^T\in \mathbb{R}^d$ 即为第 $i$ 个输入向量。

$$
\boldsymbol{X}=\begin{bmatrix}
\boldsymbol{x}_1^T \\ \vdots \\ \boldsymbol{x}_N^T
\end{bmatrix}
$$

我们希望将这组输入向量映射到一个嵌入空间，该空间能够捕捉更丰富的语义结构。假设输入向量 $\boldsymbol{x}_i$ 映射为输出向量 $\boldsymbol{y}_i$ ，则 $\boldsymbol{y}_i$ 应当取决于所有的输入向量 $\boldsymbol{x}_1,\cdots\boldsymbol{x}_N$ ，才能捕捉输入之间的依赖关系。一个很自然的想法是将 $\boldsymbol{y}_i$ 定义为所有输入向量的线性组合：

$$
\boldsymbol{y}_i=\sum_{j=1}^N\alpha_{ij}\boldsymbol{x}_j
$$

上式中的 $\alpha_{ij}$ 称为注意力系数，表示输入向量 $\boldsymbol{x}_j$ 对输出向量 $\boldsymbol{y}_i$ 的贡献程度。注意力系数应满足2个约束条件： $\alpha_{ij}\geqslant 0, \displaystyle\sum_{j=1}^N\alpha_{ij}=1$ 。

如何计算注意力系数？我们引入 查询 Query、键 Key、值 Value 的概念。想象我们在搜索引擎中搜索一部想看的电影，那么 Key 就相当于每部电影的特征，例如类型、主演、年份等，Query 就相当于观影偏好，例如“我想看一部漫威的科幻电影”，而 Value 就相当于最后搜引擎返回的结果。

引入点积自注意力机制，用点积衡量 Query $\boldsymbol{x}_j$ 和 Key $\boldsymbol{x}_i$ 之间的相似度：

除此之外，当前从输入向量到输出向量的转换时固定的，我们希望网络能够对输入向量的某些特征给予更多关注，因此通过引入一个可学习的权重矩阵 $\boldsymbol{W}$ ，定义出各自具有线性独立变换的 Query、Key、Value 矩阵（**我们常常设定它们的维度相同**）：

$$
\boldsymbol{Q}=\boldsymbol{X}\boldsymbol{W}^{(q)}= \begin{bmatrix}
\boldsymbol{q}_1^T \\ \vdots \\ \boldsymbol{q}_n
\end{bmatrix},
\boldsymbol{K}=\boldsymbol{X}\boldsymbol{W}^{(k)}= \begin{bmatrix}
\boldsymbol{k}_1^T \\ \vdots \\ \boldsymbol{k}_n
\end{bmatrix},
\boldsymbol{V}=\boldsymbol{X}\boldsymbol{W}^{(v)}= \begin{bmatrix}
\boldsymbol{v}_1^T \\ \vdots \\ \boldsymbol{v}_n
\end{bmatrix} \\
\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}\in \mathbb{R}^{N\times d},\quad
\boldsymbol{W}^{(q)},\boldsymbol{W}^{(k)},\boldsymbol{W}^{(v)}\in \mathbb{R}^{d\times d}
$$

上面的3个权重矩阵 $\boldsymbol{W}^{(q)},\boldsymbol{W}^{(k)},\boldsymbol{W}^{(v)}$ 即为最后 Transformer 架构所需要训练的可学习参数。并且通过权重矩阵 $\boldsymbol{W}$ $所有输入数据可以实现实现 **参数共享** 。

计算注意力分数 $\boldsymbol{S}=\boldsymbol{Q}\boldsymbol{K}^T \in \mathbb{R}^{N\times N}$ 。

注意力系数矩阵 $\boldsymbol{A}=\text{softmax}\left(\dfrac{\boldsymbol{Q}\boldsymbol{K}^T}{\sqrt{d}}\right) \in\mathbb{R}^{N\times N}$ 。这里我们使用了点积缩放并做 Softmax 归一化，优化数值稳定性。注意 矩阵是**每个行向量做 Softmax 运算** 。

最后把注意力系数作用在 Value 上： $\boldsymbol{Y}=\boldsymbol{A}\boldsymbol{V}=\text{softmax}\left(\dfrac{\boldsymbol{Q}\boldsymbol{K}^T}{\sqrt{d}}\right)\boldsymbol{V} ,\;\boldsymbol{Y}\in\mathbb{R}^{N\times d}$ ，相当于返回搜索结果。

输出矩阵 $\boldsymbol{Y}$ 的每一行 $\boldsymbol{y}_i$ 可以看成是 值矩阵 $\boldsymbol{V}$ 中的行向量以 注意力系数矩阵 $\boldsymbol{A}$ 为权重的线性组合。

> 输出矩阵 $\boldsymbol{Y}$ 的计算复杂度为 $O(N^2d)$ 。

对于单个输入 $\boldsymbol{x}_i$ ，其对应的 Query、Key、Value 分别为 $\boldsymbol{q}_i=\boldsymbol{x}_i\boldsymbol{W}^{(q)},\;\boldsymbol{k}_i=\boldsymbol{x}_i\boldsymbol{W}^{(k)},\;\boldsymbol{v}_i=\boldsymbol{x}_i\boldsymbol{W}^{(v)}$ 。

单个注意力系数：

$$
\alpha_{ij}=\dfrac{\boldsymbol{q}_i^T\boldsymbol{k}_j}{\sqrt{d}} \\
\hat{\alpha}_{ij}=\text{softmax}\left(\dfrac{\boldsymbol{q}_i^T\boldsymbol{k}_j}{\sqrt{d}}\right) \\
\boldsymbol{y}_i=\sum_{j=1}^N\hat{\alpha}_{ij}\boldsymbol{v}_j,\quad i,j=1,\cdots,N
$$

### 7.2.2 Multi-head Attention

上述描述的注意力层称为 注意力头 (Attention Head)，可以允许输出向量关注输入向量中与数据相关的模式。然而多数情况下，输入数据中可能存在多种不同的相关模式，例如在 NLP 中，可能要同时关注时态、词汇关系等多种信息。因此我们可以**并行**使用多个注意力头，让模型从不同角度关注输入数据的不同模式。

多头注意力机制，使用多个独立的注意力头，每个头都有自己独立的 Q,K,V 矩阵。这些头的结构完全相同，但参数是独立的。

> 这类似于 CNN 中每一个卷积层使用不同的卷积核来提取不同的特征。

假设有 $H$ 个注意力头，每个注意力头 $h$ 的 Q,K,V 矩阵为：

$$
\boldsymbol{Q}^{(h)}=\boldsymbol{X}\boldsymbol{W}^{(q,h)},\;
\boldsymbol{K}^{(h)}=\boldsymbol{X}\boldsymbol{W}^{(k,h)},\;
\boldsymbol{V}^{(h)}=\boldsymbol{X}\boldsymbol{W}^{(v,h)} \\
$$

注意力系数矩阵为：

$$
\boldsymbol{H}^{(h)}=\text{softmax}\left(\dfrac{\boldsymbol{Q}^{(h)}\boldsymbol{K}^{(h)T}}{\sqrt{d}}\right)\boldsymbol{V}^{(h)},\quad\boldsymbol{H}^{(h)}\in\mathbb{R}^{N\times d}
$$

将所有 $H$ 个头的输出拼接成一个矩阵： $\boldsymbol{H}=[\boldsymbol{H}^{(1)},\cdots \boldsymbol{H}^{(H)}]\in\mathbb{R}^{N\times Hd}$

使用一个维度为 $Hd\times d$ 的线性变换矩阵 $\boldsymbol{W}^{(o)}$ 对拼接后的矩阵变换，得到最终的多头注意力输出： $\boldsymbol{Y}=\boldsymbol{H}\boldsymbol{W}^{(o)}$ ，其中 $\boldsymbol{Y}\in\mathbb{R}^{N\times d}$ 与输入矩阵 $\boldsymbol{X}$ 维度相同。

> 输出矩阵 $\boldsymbol{Y}$ 的计算复杂度为 $O(HN^2d)$ 。

MLA (Multi-Head Latent Attention)：Deepseek-V2 提出的一种注意力架构，通过将 键 和 值 联合压缩为一个潜在向量，降低计算复杂度，也提高了全局信息提取的效率。

## 7.3 Transformer

Transformer 是完全由 注意力机制 构成的深度神经网络。注意力机制不仅能够捕获局部信息，还能建模长距离的上下文关系，获取更抽象更具全局性的特征。

Transformer 最初为 序列建模 而设计，应用于 NLP 领域。此外在 CV 领域也广泛应用，例如 Vision Transformer (ViT) 将图像划分为固定大小的 patch，将其序列化，用 Transformer 进行特征提取，替代 CNN 完成图像分类任务。

Transformer 由多个 编码器 Encoder 和 解码器 Decoder 组成。编码器和解码器都由多个相同结构的层堆叠而成，使用首尾相连的 多头注意力机制。

1. Encoder 负责对输入句子进行编码，将文本数据转换为一种抽象的表示形式，便于 Decoder 进行解码。
2. Decoder 基于 Encoder 提供的上下文信息，逐步生成目标语言的输出文本。

!!! note "计算复杂度"
    对于注意力机制，输出矩阵 $\boldsymbol{Y}$ 的计算复杂度为 $O(N^2d)$ 。如果是多头注意力机制，则为 $O(HN^2d)$ 。

    对于整个 Transformer，应考虑 注意力机制 + 前馈神经网络 MLP，总的计算复杂度为 $O(N^2d+Nd^2)$ 。

### 7.3.1 Encoder

编码器主要用于特征的提取。

> 把输入的原始句子编码为一些互相之间有交互的 embedding（对比 Word2Vec 词之间是没有交互的），所以不需要 mask。经过一个 Block 之后，输入和输出的形状是一样的，都是 $N\times d$ 。

Encoder 由 $N$ 个相同结构的编码器层 Encoder Layer 组成，每个 Encoder Layer 包含2个子层：

1. 多头自注意力机制层 Multi-head Self-attention
    - 通过 多头注意力机制 和 位置编码 捕捉输入序列中各个 token 之间的依赖关系，捕捉局部和全局依赖关系。
    - 残差连接 + Layer Normalization。
2. 前馈全连接网络层 FFN
    - 两个线性层 + ReLU 激活函数，增强非线性建模能力。
    - 残差连接 + Layer Normalization。

Encoder 是由多头自注意力模块和 FFN 模块交替堆叠而成的，且不同的 Encoder Layer 之间 **参数不共享** ，提高模型的表达能力，使得不同层可以学习到不同层次的特征。

### 7.3.2 Decoder

> ：Decoder 是希望从 embedding中 解码出新的内容，因此需要用前面的内容作为输入，在模型结尾输出新的内容。其解码过程和 RNN 比较类似，每个输出的 $o$ 再次输入 Decoder，用于后续解码。虽然输出 embedding 的形状一致，但是实际有效输出的内容与输入的形状不同。同时，Decoder 需要添加 mask。GPT 是典型的 Decoder-Only 架构。

Decoder 由 $N$ 个相同结构的解码器层 Decoder Layer 组成，每个 Decoder Layer 包含3个子层：

1. 掩码多头自注意力层 Masked Multi-head Self-attention
    - Decoder 的每个位置只关注之前生成的 token，而不能看到未来的 token。
    - 通过二值化掩码 Masking 机制，确保 Decoder 不会再预测某个单词时“偷看”后续的词，这样才能保持自回归 Auto Regressive 的特性，适用于翻译、文本生成等任务。
2. 交叉注意力层 Cross-attention
    - 允许 Decoder 关注 Encoder 提供的上下文信息。
    - 计算时，Query 来自 Decoder 的当前层，而 Key 和 Value 来自 Encoder 的输出。
3. 前馈全连接网络层 FFN
    - 两个线性层 + ReLU 激活函数。
    - 残差连接 + Layer Normalization。

### 7.3.3 FFN

前馈全连接模块(Feed-Forward Network)：

全连接层——ReLU 激活层——全连接层，默认有偏置。

![202506132354071](https://cdn.jsdelivr.net/gh/DerrickMarcus/picgo-image/images/202506132354071.png)

数学表示为：

$$
\text{FFN}(\boldsymbol{z})=\max(0, \boldsymbol{z}\boldsymbol{W}_1+\boldsymbol{b}_1)\boldsymbol{W}_2+\boldsymbol{b}_2
$$

### 7.3.4 Positional Encoding

位置编码 Positional Encoding：

RNN 的序列建模方式时位置敏感的，序列中的每个元素所在位置会影响输出结果。而自注意力机制是对位置不敏感的。一种解决方法是在输入端显式引入与位置相关的编码向量，将其和实际输入序列的编码向量一起输入到模型，这就是位置编码。Transformer 通过位置编码来捕捉输入序列中 token 的位置信息。

在语言建模中，位置编码需要体现一定的先后顺序，能反映同一单词在不同位置的区别，并且具备一定的不变性。通常选择有界周期函数，如 Sinusoidal 正余弦函数：

$$
PE(p,2i)=\sin\left(\dfrac{p}{10000^{2i/d}}\right),\quad PE(p,2i+1)=\cos\left(\dfrac{p}{10000^{2i/d}}\right)
$$

其中， $p$ 为 token 位置， $i$ 为编码向量的维度索引， $d$ 为模型的维度。

偶数维度使用 正弦函数，奇数维度使用 余弦函数。该 Sinusoidal 函数的核心是频率和维度的关系：低维频率较高，编码变化快，能够提供细粒度的位置信息；高维频率较低，编码变化慢，能够捕捉更长周期的位置信息。Sinusoidal 函数不仅能够表示绝对位置，还能通过位置之间的相对关系进行编码，使模型能够学习位置之间的相对关系。

旋转位置编码 RoPE (Rotary Position Embedding)

### 7.3.5 Layer Normalization

层归一化 Layer Normalization：

与前面的 批次归一化 Batch Normalization 不同，层归一化对每一个样本向量计算在不同特征维度上的均值和方差，利用其在维度上进行归一化和尺度变换。

> 常见的归一化方法有：batch normalization，对同一批次内数据计算均值和方差；layer normalization，对每个样本的不同维度的特征计算均值和方差；instance normalization，对每个样本的每个通道特征计算均值和方差；group normalization，对每个样本的特征按照通道分组计算均值和方差。

归一化方法对比：

| 方法 |     归一化维度     |                         优点                         |                   缺点                   |
| :--: | :----------------: | :--------------------------------------------------: | :--------------------------------------: |
|  BN  |     各通道批次     | 直接利用跨样本统计信息，在 batch size 较大时效果显著 | 在 batch size 较小时，估计的统计值不稳定 |
|  LN  |   单样本所有特征   |      不依赖 batch size，适应变长数据和动态结构       |    对通道相关性敏感，可能丢失局部特征    |
|  IN  |  单样本单通道特征  |       不依赖 batch size，更多地保留特征独特性        |    忽略通道间关系，可能不适合分类任务    |
|  GN  | 单样本分组通道特征 |   平衡通道与空间关系，在 batch size 较小时维持稳定   |      需手动设置分组数目，超参数敏感      |

## 7.4 Application

应用：语言模型，机器翻译

### 7.4.1 LLM

BERT: Bidirectional Encoder Representations from Transformers (Google)

架构：双向 Transformer，可以同时学习文本中的前后文信息。

预训练任务：遮挡语言模型(Masked Language Modeling)和下一句预测(Next Sentence Prediction)。前者通过在输入文本中随机遮挡一些词汇并预测被遮挡的词，帮助模型学会理解双向上下文（完形填空）；后者则让模型学会判断两个句子是否是连续的。

优势：文本中的双向关系理解。

适用场景：适用于自然语言理解任务，如情感分析、命名实体识别、问答等。通常作为一个特征提取器来为下游任务提供表示。

---

GPT: Generative Pre-trained Transformer (OpenAI)

架构：从左到右的单向 Transformer 模型，主要关注当前词的左侧上下文。

预训练任务：语言预测建模，从左到右的语言模型任务进行预训练。

优势：自回归生成模型，适合自然语言生成（Natural Language Generation, NLG）任务，如机器翻译、对话生成、文本摘要等。

适用场景：连贯生成文本，因为它从左到右地生成文本，可以保持生成内容的连贯性。

---

Vision Transformer (ViT).

Segmentation Transformer.

Sora.

SAM: Segment anything.

视觉-语言模型 VLM。

## 7.5 Technology

大规模预训练：

- 使用与下游任务无关的大规模数据进行模型参数的初始训练，可以认为是为模型参数找到一个较好的“初值点”。
- 大语言模型的能力基础主要来源于预训练数据，数据的收集与清洗对于模型性能具有重要的影响。

指令微调与人类对齐：

- 使用任务输入与输出的配对数据进行模型训练，使语言模型较好地掌握通过问答形式进行任务求解的能力。
- 指令微调很难教会大语言模型预训练阶段没有学习到的知识与能力，主要起到对于模型能力的激发作用，而不是知识注入作用。
- 与预训练相比，指令微调通常来说需要的指令实例数据规模要小的多。

数据预处理。为了确保数据的质量和效用，还需要对数据进行预处理：

- 质量过滤：去除语料库中的低质量数据。
- 敏感内容过滤：过滤和处理数据中的有毒内容或隐私信。
- 数据去重：去除训练数据中的重复模式，避免过度学习。
- 词元化(Tokenization)旨在将原始文本分割成模型可识别和建模的词元序列。

Tokenization：

1. BPE(Byte Pair Encoder)分词
    - 合并：从一组基本符号（例如字母表）开始，迭代地寻找语料库中的两个相邻词元，并将它们替换为新的词元。
    - 合并的选择标准是计算两个连续词元的共现频率，一直持续达到预定义的词表大小。
2. WordPiece分词
    - 和BPE分词的做法相似，但合并的选择策略有不同。
    - 合并前，先训练一个语言模型，对所有可能的词元对进行评分。
    - 每次合并，选择使得训练数据的似然性增加最多的词元对。
3. Unigram分词
    - 从一组足够大的初始集合开始，迭代地删除其中的词元，直到达到预期的词表大小。
    - 选择标准是删除某个词元后，训练语料的似然增加情况。

语言模型的架构：

1. Encoder: BERT, RoBerta, Reformer, FlauBERT, CamemBERT, Electra, MobileBERT, Longformer.
2. Decoder: Transformer-XL, XLNet, GPT series, DialoGPT.
3. Encoder-Decoder: Transformer, XLM, T5, BART, XLM-RoBerta, Pegasus, mBART.

---

Scaling Law：在大模型训练中，如何以较小的计算代价(computational cost)预测模型性能(performance)？

影响模型性能的因素：模型大小 model size，数据规模 data size，训练计算量 training compute budget。

自监督学习 Self-supervised learning

- 无监督学习的特殊形式.
- 对于无标签数据，通过设计辅助任务来挖掘数据自身的表征特征作为监督信号，从而提升模型的特征提取能力。
- 从大规模的无监督数据中挖掘自身的监督信息，通过构造的监督信息对网络进行训练，可以学习到对下游任务有价值的表征。

在进行模型的大规模预训练时，需要设计合适的自监督预训练任务。

LLM 常用的预训练任务：语言建模（Language Modeling, LM），去噪自编码（Denoising Autoencoding, DAE），混合去噪器（Mixture-of-Denoisers, MoD）。

---

指令微调 Instruction Tuning 是指使用自然语言形式的数据，对预训练后的大语言模型进行参数微调。

低秩适配 Low-Rank Adaptation (LoRA) 微调方法，预训练模型的参数矩阵上添加低秩分解矩阵，来近似每层的参数更新，从而减少所需训练的参数。

适配器微调 Adapter Tuning：在 Transformer 模型中引入小型神经网络模块（适配器），首先将原始特征压缩到较低维度，随后进行非线性变换，最后恢复到原始维度。适配器模块集成到 Transformer 架构的每一层中，原始的模型参数保持不变。

前缀微调 Prefix Tuning：在每个多头注意力层中添加了一组前缀参数，可视为若干虚拟词元的嵌入向量。

提示微调：仅在输入嵌入层中加入可训练的提示向量。

为什么进行人类对齐？

经过大规模的预训练和有监督指令微调，大语言模型具备了解决各种任务的通用能力和指令遵循能力。但是同时也可能生成有偏见的、冒犯的以及事实错误的文本内容。因此，在大语言模型的学习过程中，如何确保大语言模型的行为与人类价值观、人类真实意图和社会伦理相一致成为了一个关键研究问题，通常称这一研究问题为人类对齐。

基于人类反馈的强化学习 RLHF (Reinforcement Learning from Human Feedback)，3个阶段：

1. 指令微调 SFT。
2. 奖励模型训练：使用人类反馈数据训练奖励模型，使用语言模型针对任务指令生成一定数量的候选输出，邀请标注员对于输出文本进行偏好标注，使用人工标注的偏好数据进行奖励模型的训练。
3. 强化学习微调：语言模型对齐被转化为一个强化学习问题。

强化学习 RL (Reinforcement Learning)

1. 智能体(agent)与环境(environment)交互。
2. 根据环境的状态(state)通过学习产生行动(action)的策略。
3. 实现奖励(reward)最大化或其他特定目标。

---

提示 Prompt 学习

提示，通常为包含特定任务信息的一组人工设计的输入或者可学习的特征向量，用于引导模型从输入文本或图像中提取特定任务所需要的信息。通过选择或生成合适的提示，引导模型适应多样化的输入和下游任务。

Prompt Engineering 提示工程 ，是针对特定任务构造能充分发挥大模型能力的 Prompt：

1. 大语言模型的微调代价较高。
2. 提示的质量在很大程度上会影响LLM在特定任务中的表现。
