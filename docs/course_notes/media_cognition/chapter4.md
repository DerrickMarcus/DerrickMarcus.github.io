# 4 Deep Learning æ·±åº¦å­¦ä¹ 

## 4.1 Introduction

æµ…å±‚å­¦ä¹ (Shallow Learning)ï¼šä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œä¿¡å·å¤„ç†ä»…å«å•å±‚éçº¿æ€§å˜æ¢ï¼Œç§°ä¸ºæµ…å±‚å­¦ä¹ ç»“æ„ã€‚ä¾‹å¦‚æ„ŸçŸ¥å™¨æ¨¡å‹ï¼Œçº¿æ€§åˆ¤åˆ«åˆ†æï¼Œæ”¯æŒå‘é‡æœºï¼Œéšé©¬å°”å¯å¤«æ¨¡å‹ï¼Œæ¡ä»¶éšæœºåœºã€‚å®ƒä»¬å¯¹å¤æ‚å‡½æ•°è¡¨ç¤ºèƒ½åŠ›æœ‰é™ï¼Œå¯¹å¤æ‚åˆ†ç±»é—®é¢˜æ€§èƒ½å—é™ã€‚

æ·±åº¦å­¦ä¹ (Deep Learning)ï¼šå—å¤§è„‘çš„åˆ†å±‚ç»“æ„å¯å‘ï¼Œåˆ©ç”¨å¤šä¸ªéšå±‚çš„äººå·¥ç¥ç»ç½‘ç»œèµ‹äºˆç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼›é€šè¿‡å­¦ä¹ æ·±å±‚éçº¿æ€§ç½‘ç»œç»“æ„ï¼Œå®ç°å¤æ‚å‡½æ•°çš„é€¼è¿‘ï¼Œä»æ•°æ®ä¸­å­¦ä¹ æœ¬è´¨ç‰¹å¾ã€‚

æ·±åº¦å­¦ä¹ å¼ºè°ƒæ¨¡å‹ç»“æ„çš„æ·±åº¦ï¼Œé€šå¸¸æœ‰5å±‚ä»¥ä¸Šçš„éšå±‚èŠ‚ç‚¹ã€‚æ·±åº¦å­¦ä¹ é€šè¿‡**é€å±‚ç‰¹å¾å˜æ¢**ï¼Œå°†æ ·æœ¬ä»åŸç‰¹å¾ç©ºé—´å˜æ¢åˆ°æ–°çš„ç‰¹å¾ç©ºé—´ï¼Œä»è€Œä½¿åˆ†ç±»é¢„æµ‹æ›´åŠ å®¹æ˜“ï¼›åˆ©ç”¨**å¤§æ•°æ®**æ¥å­¦ä¹ ç‰¹å¾ï¼Œå®ç°å¯¹æ¨¡å¼æœ¬è´¨ç‰¹å¾çš„è‡ªåŠ¨å­¦ä¹ ã€‚

æ·±åº¦å­¦ä¹ çš„å‘å±•å†å²ï¼šç¥ç»ç½‘ç»œæ¦‚å¿µåŠäººå·¥ç¥ç»å…ƒçš„æ•°å­¦æ¨¡å‹(1943)ï¼Œç¥ç»é›†åˆä½“å‡è®¾(1949)ï¼Œæ„ŸçŸ¥å™¨æ¨¡å‹ Perceptron (1957)ï¼ŒBP ç¥ç»ç½‘ç»œç®—æ³•ï¼ŒNeocognitron æ¨¡å‹(1980)ï¼Œå·ç§¯ç¥ç»ç½‘ç»œ CNNï¼Œé•¿çŸ­æ—¶è®°å¿†ç½‘ç»œ LSTM (1997)ï¼Œæ·±åº¦ç½®ä¿¡ç½‘ç»œ DBN (2006)ï¼Œè‡ªç¼–ç å™¨ Auto encoder (2006)ï¼Œç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œ GAN (2014)ï¼ŒSelf-attention & Transformer (2017)ã€‚

## 4.2 äººå·¥ç¥ç»å…ƒæ¨¡å‹

å›é¡¾æœºå™¨å­¦ä¹ ä¸­çš„æ„ŸçŸ¥æœº Perceptionï¼Œæ„ŸçŸ¥æœºä¸€ç§æœ€ç®€å•å½¢å¼çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå¯åº”ç”¨äºäºŒå…ƒçº¿æ€§åˆ†ç±»ã€‚é€»è¾‘å›å½’ Logistic Regression æ˜¯è§£å†³äºŒåˆ†ç±»é—®é¢˜çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºä¼°è®¡æŸç§äº‹ç‰©çš„å¯èƒ½æ€§ã€‚

äººå·¥ç¥ç»å…ƒæ¨¡å‹(neuron)ï¼šç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒ

1. è¾“å…¥ï¼šæ¥å—å¤–ç•Œæˆ–è€…å‰å±‚è¾“å…¥ã€‚
2. è¿æ¥ï¼šæ ¹æ®æƒé‡å¯¹è¾“å…¥åŠ æƒã€‚
3. æ¿€æ´»å‡½æ•°ï¼šè¿æ¥å±‚è¾“å‡ºçš„éçº¿æ€§æ˜ å°„ã€‚
4. è¾“å‡ºï¼šè¾“å‡ºè‡³ä¸‹ä¸ªéšå«å±‚ã€‚

å…³äº**æ¿€æ´»å‡½æ•°**çš„æ€»ç»“ï¼Œå¯è§ [ç¬¬3è®² æœºå™¨å­¦ä¹ -æ„ŸçŸ¥æœº-æ¿€æ´»å‡½æ•°](./chapter3.md#course_notes/media_cognition/section-3.2.1)ã€‚

æŸå¤±å‡½æ•°(Loss Function)ï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥è¯„ä»·æ¨¡å‹ä¼˜åŠ£ã€‚æŸå¤±å‡½æ•°ä¸€èˆ¬æ˜¯éè´Ÿçš„ï¼Œæœ‰ä¸‹ç•Œã€‚

å¤šåˆ†ç±»é—®é¢˜ä¸­çš„æŸå¤±å‡½æ•°ï¼š**Softmax æ¿€æ´» + è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±**

$$
L=-\sum_{i} \log\left(\frac{\exp(z_{i,j})}{\displaystyle\sum_{k} \exp(z_{i,k})}\right)
$$

ä¸Šå¼ä¸­ $j$ ä¸ºç¬¬ $i$ ä¸ªæ ·æœ¬ $x_i$ çš„çœŸå®ç±»åˆ«æ ‡ç­¾ï¼Œ $z_{i,k}$ ä¸ºè¾“å‡ºçš„é¢„æµ‹å‘é‡ã€‚

å‡è®¾æœ‰ $K$ ä¸ªç±»åˆ«ï¼ŒæŸæ ·æœ¬è¾“å‡º $K$ ç»´å‘é‡ $\boldsymbol{z}_i = [z_{i,1}, z_{i,2}, \cdots, z_{i,K}]^T$ ï¼Œåˆ™å…¶å±äºç¬¬ $j$ ç±»çš„æ¦‚ç‡ä¸º $\hat{y}_{i,j} = \dfrac{\exp(z_{i,j})}{\sum_{k=1}^{K} \exp(z_{i,k})}$ ï¼›è‹¥æ ·æœ¬çš„çœŸå®ç±»åˆ«æ ‡ç­¾ä¸ºç¬¬ $j$ ç±»ï¼Œåˆ™ $\hat{y}_{i,j}$ è¶Šå¤§è¶Šæ¥è¿‘äº 1 åˆ™æŸå¤±è¶Šå°è¶Šæ¥è¿‘äº 0ï¼›åä¹‹æŸå¤±è¶Šå¤§ã€‚

<br>

å›å½’é—®é¢˜ä¸­çš„æŸå¤±å‡½æ•°ï¼š

ï¼ˆ1ï¼‰å‡æ–¹æŸå¤± $L=\dfrac{1}{2} \displaystyle\sum_{i}\|y_i - \hat{y}_i\|^2,\;\hat{y}_i=h(\boldsymbol{x}_i)$ ï¼Œå…¶ä¸­ $y_i$ ä¸ºçœŸå®å€¼ï¼Œ $\hat{y}_i$ ä¸ºé¢„æµ‹å€¼ã€‚

ï¼ˆ2ï¼‰L1 æŸå¤± $L=\displaystyle\sum_{i}\|y_i - \hat{y}_i\|$ ï¼Œå…¶ä¸­ $y_i$ ä¸ºçœŸå®å€¼ï¼Œ $\hat{y}_i$ ä¸ºé¢„æµ‹å€¼ã€‚

ï¼ˆ3ï¼‰Smooth-L1 æŸå¤±ï¼š

$$
L_i=\begin{cases}
\dfrac{1}{2}(y_i - \hat{y}_i)^2 & \text{if } |y_i - \hat{y}_i| < 1 \\
|y_i - \hat{y}_i| - \dfrac{1}{2} & \text{otherwise}
\end{cases},\quad L=\sum_{i} L_i
$$

## 4.3 FFN

å…¨è¿æ¥å‰é¦ˆç¥ç»ç½‘ç»œ(Fully-connected FeedForward Networks, FFN)ã€‚

åœ¨ç½‘ç»œç»“æ„ä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒæœ‰å¯¹åº”æƒé‡å’Œåç½®å‚æ•°ï¼Œæ‰€æœ‰ç¥ç»å…ƒçš„æƒé‡å’Œåç½®å®šä¹‰ä¸ºç½‘ç»œå‚æ•°ã€‚

è‹¥ç›¸é‚»ä¸¤å±‚çš„ç¥ç»å…ƒçš„æ•°é‡åˆ†åˆ«ä¸º $n_1,n_2$ ï¼Œåˆ™ä¸¤å±‚ç¥ç»å…ƒä¹‹é—´çš„ç½‘ç»œï¼Œæƒé‡çš„å‚æ•°é‡ä¸º $n_1n_2$ ï¼Œåç½®çš„å‚æ•°é‡ä¸º $n_2$ ï¼Œå› æ­¤ä¸¤å±‚ç¥ç»å…ƒä¹‹é—´çš„å‚æ•°é‡ä¸º $n_1n_2+n_2$ . æ•´ä¸ªç¥ç»ç½‘ç»œçš„å‚æ•°é‡ä¸ºæ‰€æœ‰å±‚ä¹‹é—´çš„å‚æ•°é‡ä¹‹å’Œï¼Œæœ‰ $N$ å±‚åˆ™æ±‚å’Œ $N-1$ æ¬¡ã€‚

> è¿™ä¸€ç‚¹å¾ˆå®¹æ˜“ç†è§£ï¼Œå› ä¸ºå•å±‚ç½‘ç»œçš„è¾“å‡ºå½¢å¦‚ $\mathbb{R}^{n_1}\to\mathbb{R}^{n_2}:\;\boldsymbol{y}=\boldsymbol{w}^T\boldsymbol{x}+\boldsymbol{b},\;\boldsymbol{w}\in\mathbb{R}^{n_1\times n_2},\;\boldsymbol{b}\in\mathbb{R}^{n_2}$ .

### 4.3.1 BP

Back Propagation (BP) å­¦ä¹ ç®—æ³•

æœ€å¸¸ç”¨çš„ç¥ç»ç½‘ç»œçš„ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œå…¶æ•°å­¦åŸºç¡€æ˜¯**é“¾å¼æ±‚å¯¼æ³•åˆ™**ã€‚BP å­¦ä¹ ç®—æ³•ç”±å‰å‘ä¼ æ’­å’Œè¯¯å·®åå‘ä¼ æ’­ç»„æˆï¼š

**å‰å‘ä¼ æ’­**æ˜¯è¾“å…¥ä¿¡å·ä»è¾“å…¥å±‚ç»éšå«å±‚ï¼Œä¼ å‘è¾“å‡ºå±‚ã€‚è‹¥è¾“å‡ºå±‚å¾—åˆ°äº†æœŸæœ›çš„è¾“å‡ºï¼Œåˆ™å­¦ä¹ ç®—æ³•ç»“æŸï¼›å¦åˆ™ï¼Œè½¬è‡³åå‘ä¼ æ’­ã€‚

**åå‘ä¼ æ’­**æ˜¯å°†è¯¯å·®ï¼ˆæ ·æœ¬è¾“å‡ºä¸ç½‘ç»œè¾“å‡ºä¹‹å·®ï¼‰æŒ‰åŸè”æ¥é€šè·¯åå‘è®¡ç®—ï¼Œç”±**æ¢¯åº¦ä¸‹é™æ³•**è°ƒæ•´å„å±‚èŠ‚ç‚¹çš„æƒå€¼å’Œé˜ˆå€¼ï¼Œä½¿è¯¯å·®å‡å°ã€‚

!!! warning "è¯´æ˜"
    å‰ä¼ å’Œåä¼ ã€æ¢¯åº¦çš„è®¡ç®—è¿‡ç¨‹éœ€è¦ç»“åˆå…·ä½“çš„ç¥ç»ç½‘ç»œå›¾åƒæ‰èƒ½è¾ƒå¥½è®²è§£ï¼Œå› æ­¤æ­¤å¤„ç•¥è¿‡ã€‚

    å¯¹äºç®€å•çš„ç½‘ç»œï¼Œä¾‹å¦‚è€ƒè¯•é¢˜ï¼Œå¯ä»¥å¯¹å‚æ•°é€ä¸ªæ‰‹åŠ¨è®¡ç®—ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨çŸ©é˜µå½¢å¼è®¡ç®—ï¼Œä½†æ˜¯è¦æ³¨æ„çŸ©é˜µçš„æ±‚å¯¼çš„æ–¹æ³•ã€‚

å‰å‘ä¼ æ’­ï¼š

$$
\begin{align*}
\boldsymbol{z}^{(l)} &= \boldsymbol{W}^{(l)}\boldsymbol{a}^{(l-1)} + \boldsymbol{b}^{(l)},&\quad \boldsymbol{a}^{(l)} &= f(\boldsymbol{z}^{(l)}) \\
\frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{W}^{(l)}} &= \boldsymbol{a}^{(l-1)},&\quad \frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}} &= 1
\end{align*}
$$

> ä¸å‰é¢è®¨è®ºçš„å‘é‡ $\boldsymbol{w}$ ä¸åŒï¼Œå…¨è¿æ¥å±‚çš„æƒé‡ $\boldsymbol{W}$ æ˜¯ä¸€ä¸ªäºŒç»´çŸ©é˜µï¼Œå› æ­¤æ¯ä¸€å±‚çš„è¾“å‡ºå½¢å¦‚ $\boldsymbol{y}=\boldsymbol{W}\boldsymbol{x}+\boldsymbol{b}$ ï¼Œæƒé‡ä¸éœ€è¦åšè½¬ç½®ã€‚

<br>

åå‘ä¼ æ’­ï¼ˆé€’æ¨è¡¨è¾¾å¼ï¼‰ï¼š

$$
\begin{gather*}
\frac{\partial L}{\partial \boldsymbol{z}^{(l)}}=\delta^{(l)} = (\boldsymbol{W}^{(l+1)})^T \odot f'(\boldsymbol{z}^{(l)}) \delta^{(l+1)} \\
\frac{\partial L}{\partial \boldsymbol{W}^{(l)}} = \delta^{(l)}(\boldsymbol{a}^{(l-1)})^T,\quad \frac{\partial L}{\partial \boldsymbol{b}^{(l)}} = \delta^{(l)}
\end{gather*}
$$

åˆ™æŸå¤±å‡½æ•° å¯¹ ç¬¬ $l$ å±‚æƒé‡çŸ©é˜µ $\boldsymbol{W}^{(l)}$ ä¸­ç¬¬ $(i,j)$ ä¸ªæƒé‡ $\boldsymbol{W}_{ij}^{(l)}$ çš„æ¢¯åº¦ä¸ºï¼š

$$
\frac{\partial L}{\partial \boldsymbol{W}_{ij}^{(l)}} = \frac{\partial L}{\partial z_i^{(l)}} \cdot \frac{\partial z_i^{(l)}}{\partial \boldsymbol{W}_{ij}^{(l)}} = \delta_i^{(l)} \cdot a_j^{(l-1)}
$$

---

ç®€å•æ€»ç»“**ä½¿ç”¨çŸ©é˜µæ±‚å¯¼æ–¹æ³•åš BP**ï¼Œå¯èƒ½ä¼šæ¯”é€ä¸ªæƒé‡å…ƒç´ è®¡ç®—æ›´ç®€å•å¿«æ·ã€‚è¿™ç±»é¢˜ç›®ä¸ä¼šå‡ºçš„å¾ˆéš¾ï¼Œå› ä¸ºçŸ©é˜µã€å‘é‡ä¹‹é—´åƒå˜ä¸‡åŒ–ã€è¿‡äºå¤æ‚ï¼Œè€Œé€ä¸ªå…ƒç´ è®¡ç®—æ¢¯åº¦ä¹Ÿå¾ˆå®¹æ˜“è®¡ç®—é‡çˆ†ç‚¸ï¼Œå› æ­¤æŒæ¡åŸºæœ¬çš„æ±‚å¯¼å…¬å¼å°±èƒ½åº”ä»˜å¤§éƒ¨åˆ†é¢˜ç›®ã€‚

ç»å…¸çš„é“¾å¼æ±‚å¯¼å…¬å¼ï¼š

$$
\frac{\partial g(f(\boldsymbol{x}))}{\partial \boldsymbol{x}} = \frac{\partial g}{\partial f} \cdot \frac{\partial f}{\partial \boldsymbol{x}}
$$

æœ‰3ç§å¸¸è§çš„æ±‚å¯¼ï¼š

1. æ¿€æ´»å‡½æ•°æ±‚å¯¼ï¼š $\mathbb{R}^N \to \mathbb{R}^N$ .
2. çŸ©é˜µæ±‚å¯¼ã€‚
3. Loss æ±‚å¯¼ï¼š $\mathbb{R}^N \to \mathbb{R}$ .

å¯¹äº**æ¿€æ´»å‡½æ•°æ±‚å¯¼**ï¼Œç”±äºæ˜¯å¯¹å‘é‡å„ä¸ªå…ƒç´ ä½œç”¨ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨ Hadamard ä¹˜ç§¯ï¼ˆé€å…ƒç´ ç›¸ä¹˜ï¼‰ï¼Œä»¥ Sigmoid å‡½æ•°ä¸ºä¾‹ï¼š

$$
\begin{align*}
\sigma'(x) &= \frac{e^{-x}}{(1 + e^{-x})^2} = \sigma(x)(1 - \sigma(x)),\quad x\in\mathbb{R} \\
\frac{\partial \sigma(\boldsymbol{x})}{\partial \boldsymbol{x}} &= \sigma(\boldsymbol{x}) \odot (1 - \sigma(\boldsymbol{x})) ,\quad \boldsymbol{x}\in\mathbb{R}^N
\end{align*}
$$

å¯¹äº**çŸ©é˜µ/å‘é‡æ±‚å¯¼**ï¼Œæœ‰ä»¥ä¸‹å¸¸ç”¨å…¬å¼ï¼š

$$
\begin{gather*}
\dfrac{\partial \boldsymbol{W} \boldsymbol{x}}{\partial \boldsymbol{W}} = \boldsymbol{x}^T ,\quad \dfrac{\partial \boldsymbol{W} \boldsymbol{x}}{\partial \boldsymbol{x}} = \boldsymbol{W}^T \\
\dfrac{\partial L}{\partial \boldsymbol{W}} = \dfrac{\partial L}{\partial (\boldsymbol{W}\boldsymbol{x})} \boldsymbol{x}^T ,\quad \dfrac{\partial L}{\partial \boldsymbol{x}}=\boldsymbol{W}^T \dfrac{\partial L}{\partial (\boldsymbol{W}\boldsymbol{x})} \\
\|\boldsymbol{A}\|_2^2 = \text{trace}(\boldsymbol{A}^T \boldsymbol{A}) \\
\dfrac{\partial \,\text{trace}(\boldsymbol{A}^T \boldsymbol{B})}{\partial \boldsymbol{A}} = \boldsymbol{B} ,\quad \dfrac{\partial \,\text{trace}(\boldsymbol{A} \boldsymbol{B})}{\partial \boldsymbol{B}} = \boldsymbol{A}^T \\
\text{trace}(\boldsymbol{A} \boldsymbol{B} \boldsymbol{C}) = \text{trace}(\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}) = \text{trace}(\boldsymbol{B} \boldsymbol{C} \boldsymbol{A}) \\
\text{trace}(\boldsymbol{A}) = \text{trace}(\boldsymbol{A}^T) \\
\text{trace}(\boldsymbol{A} + \boldsymbol{B}) = \text{trace}(\boldsymbol{A}) + \text{trace}(\boldsymbol{B})
\end{gather*}
$$

æ ‡é‡ å¯¹ çŸ©é˜µ/å‘é‡ æ±‚å¯¼ï¼š $\dfrac{\partial \mathbb{R}^{1\times 1}}{\partial \mathbb{R}^{m \times n}}\to \mathbb{R}^{m \times n}$ ï¼Œæ ‡é‡å¯¹ä»»ä½•çŸ©é˜µ/å‘é‡æ±‚å¯¼ï¼Œç»“æœçš„ç»´åº¦ éƒ½ä¸ è¢«æ±‚å¯¼çš„çŸ©é˜µ/å‘é‡ ç›¸åŒã€‚

å‘é‡ å¯¹ å‘é‡ æ±‚å¯¼ï¼š $\dfrac{\partial \mathbb{R}^{m\times 1}}{\partial \mathbb{R}^{n \times 1}}\to \mathbb{R}^{m \times n}$ .

çŸ©é˜µ å¯¹ çŸ©é˜µ æ±‚å¯¼ï¼š $\dfrac{\partial \mathbb{R}^{m\times n}}{\partial \mathbb{R}^{p \times q}}\to \mathbb{R}^{mn \times pq}$ . ä¸¥æ ¼æ¥è®²â€œçŸ©é˜µå¯¹çŸ©é˜µâ€å¯¼æ•°æœ¬è´¨æ˜¯ 4 é˜¶å¼ é‡ï¼Œä½†æ˜¯ç»å¸¸ reshape æˆäºŒç»´çŸ©é˜µã€‚è¿™ç§æƒ…å†µä¸å¸¸è§ã€‚

!!! tip
    å¯¼æ•°å’Œè¢«å¯¼æ•°å¯ä»¥åŒæ—¶è½¬ç½®ã€‚

<br>

å¯¹äº **Loss æ±‚å¯¼**ï¼Œæœ¬è´¨æ˜¯ æ ‡é‡ å¯¹ çŸ©é˜µ/å‘é‡ æ±‚å¯¼ï¼Œæˆ‘ä»¬åªéœ€è¦æŒæ¡ MSE å’Œäº¤å‰ç†µä¸¤ç§ï¼š

$$
\begin{align*}
\text{MSE:} \quad L(\boldsymbol{y},\boldsymbol{t}) &= \frac{1}{2} \|\boldsymbol{y}-\boldsymbol{t}\|^2 ,\quad
\frac{\partial \|\boldsymbol{y}-\boldsymbol{t}\|^2}{\partial \boldsymbol{y}} = 2(\boldsymbol{y}-\boldsymbol{t}) \\
\text{Cross Entropy:} \quad L(\boldsymbol{y},\boldsymbol{t}) &= -\sum_{i=1}^{N} t_i \log(y_i) ,\quad \frac{\partial L}{\partial \boldsymbol{y}} = -\frac{\boldsymbol{t}}{\boldsymbol{y}}
\end{align*}
$$

ä¸Šå¼ä¸­ $\boldsymbol{t}$ ä¸ºçœŸå®å€¼ï¼Œ $\boldsymbol{y}$ ä¸ºé¢„æµ‹å€¼ï¼Œå®ƒä»¬åš**é€å…ƒç´ é™¤æ³•**ã€‚

!!! bug "æ³¨æ„"
    å¦‚æœå®Œå…¨æŒ‰ç…§ä¸Šé¢çš„æ–¹æ³•ï¼Œé‚£ä¹ˆé“¾å¼æ±‚å¯¼ å„é¡¹ç›¸ä¹˜çš„æ—¶å€™ï¼Œå¾ˆå¯èƒ½ä¼šå‡ºç°ç»´åº¦ä¸åŒ¹é…çš„ç°è±¡ï¼Œå¾ˆæ­£å¸¸ï¼Œè¿™æ—¶å€™å°±éœ€è¦éšæœºåº”å˜äº†ğŸ˜‚ã€‚å¦å¤–ï¼Œæœ€å¥½ä¸è¦ä¸€æ­¥å†™åˆ°ä½ï¼Œä»åå¾€å‰ä¸€æ­¥ä¸€æ­¥æ¥ï¼Œæ¯ä¸€æ­¥ä½¿ç”¨æ·»åŠ åˆé€‚çš„è½¬ç½®ç­‰æ–¹æ³•ï¼Œä¿è¯ä¸­é—´ç»“æœæ¯ä¸€æ­¥éƒ½æ˜¯å¯¹çš„ã€‚

---

è®­ç»ƒæ–¹æ³•ï¼šæ¢¯åº¦ä¸‹é™

ç¥ç»ç½‘ç»œå…¨éƒ¨å‚æ•° $\theta=\{\boldsymbol{W}_1,\cdots,\boldsymbol{b}_1,\cdots\}$ ï¼Œè®­ç»ƒç›®æ ‡æ˜¯å­¦ä¹ è·å–ä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–çš„ç½‘ç»œå‚æ•° $\theta^*$ ï¼Œå‚æ•°æ›´æ–°è§„åˆ™ä¸ºï¼š

$$
w \leftarrow w - \eta \frac{\partial L}{\partial w},\quad b \leftarrow b - \eta \frac{\partial L}{\partial b} ,\quad
\theta \leftarrow \theta - \eta \nabla_\theta L(\theta)
$$

æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™ BGD (Batch Gradient Descent)ï¼Œæ‰€æœ‰æ ·æœ¬éƒ½å‚ä¸è®¡ç®—æ¢¯åº¦ï¼š $\theta \leftarrow \theta - \eta \nabla_\theta L(\theta)$ ï¼Œç¼ºç‚¹æ˜¯é€Ÿåº¦æ…¢ï¼Œæ•°æ®é‡å¤§æ—¶å†…å­˜ä¸è¶³ã€‚

éšæœºæ¢¯åº¦ä¸‹é™ SGD (Stochastic Gradient Descent)ï¼Œæ¯æ¬¡åªç”¨ä¸€ä¸ªæ ·æœ¬å‚ä¸è®¡ç®—æ¢¯åº¦ï¼š $\theta \leftarrow \theta - \eta \nabla_\theta L(\theta,x_i,y_i)$ ï¼Œä¼˜ç‚¹æ˜¯é€Ÿåº¦å¿«ï¼Œç¼ºç‚¹æ˜¯æ–¹å·®å¤§ï¼ŒæŸå¤±å‡½æ•°éœ‡è¡ä¸¥é‡ã€‚

å°æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™ Mini-batch Gradient Descent (M-SGD)ï¼Œä»‹äº BGD å’Œ SGD ä¹‹é—´ï¼Œæ¯æ¬¡éšæœºé€‰å– $M$ ä¸ªæ ·æœ¬å‚ä¸è®¡ç®—æ¢¯åº¦ï¼š $\theta \leftarrow \theta - \eta\left[\dfrac{1}{M}\displaystyle\sum_{i=1}^M\nabla_\theta L(\theta,x_i,y_i)\right]$ .

3ç§æ–¹æ³•çš„æ¯”è¾ƒï¼š

|      ç‰¹æ€§      |    BGD     |   SGD    |        M-SGD         |
| :------------: | :--------: | :------: | :------------------: |
| å•æ¬¡è¿­ä»£æ ·æœ¬æ•° | æ•´ä¸ªæ•°æ®é›† | å•ä¸ªæ ·æœ¬ | æ•´ä¸ªæ•°æ®é›†çš„ä¸€ä¸ªå­é›† |
|   ç®—æ³•å¤æ‚åº¦   |     é«˜     |    ä½    |         ä¸€èˆ¬         |
|     æ—¶æ•ˆæ€§     |     ä½     |   ä¸€èˆ¬   |         ä¸€èˆ¬         |
|     æ”¶æ•›æ€§     |    ç¨³å®š    |  ä¸ç¨³å®š  |        è¾ƒç¨³å®š        |

è®­ç»ƒæµç¨‹ï¼š

1. åˆå§‹åŒ–ç¥ç»ç½‘ç»œï¼Œåˆå§‹åŒ–è®¾ç½®ç½‘ç»œå‚æ•°ã€‚
2. å‰å‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦ï¼Œåå‘ä¼ æ’­ã€‚
3. é‡å¤ï¼Œç›´åˆ°æ¢¯åº¦çš„æ›´æ–°éå¸¸å°ã€‚

### 4.3.2 Optimization

æ¢¯åº¦ä¸‹é™å¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼šèƒ½æ‰¾åˆ°å±€éƒ¨æœ€ä¼˜ï¼Œä½†æ˜¯æ— æ³•ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜ã€‚å› ä¸ºå¯èƒ½é‡åˆ°éç‚¹ (Saddle Point)ã€‚æ”¹è¿›æ–¹æ³•æœ‰ï¼š

ï¼ˆ1ï¼‰**åŠ¨é‡** Momentum

é¿å…éšæœºæ¢¯åº¦ä¸‹é™é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚ç»´æŠ¤ä¸€ä¸ªâ€œçŠ¶æ€å˜é‡â€ $\beta$ è®°å½•ä¹‹å‰çš„æ¢¯åº¦ï¼Œæ¯æ¬¡æ›´æ–°å‚æ•°æ—¶ä¸ä»…è€ƒè™‘å½“å‰çš„æ¢¯åº¦ï¼Œä¹Ÿè€ƒè™‘ä¹‹å‰å­˜ä¸‹æ¥çš„åŠ¨é‡ï¼Œä»è€Œå¸®åŠ©æ¨¡å‹è¶Šè¿‡ä¸Šé¢çš„å±€éƒ¨æœ€å°å€¼ã€‚

$$
\beta_{t+1} = \mu \beta_t - \eta \nabla_{\theta} L(\theta_t), \quad \theta_{t+1} = \theta_t + \beta_{t+1}
$$

å…¶ä¸­ $\beta_t$ å³ä¸ºåŠ¨é‡ï¼Œ $\mu\in[0,1]$ ä¸ºå¯¹åº”çš„å¸¸ç³»æ•°ã€‚å‚æ•°ä¸­æ¢¯åº¦æ–¹å‘ä¸å¤§çš„ç»´åº¦åŠ é€Ÿæ›´æ–°ï¼ŒåŒæ—¶å‡å°‘åœ¨æ¢¯åº¦æ–¹å‘å˜åŒ–å¤§çš„ç»´åº¦ä¸Šçš„æ›´æ–°å¹…åº¦ã€‚

ï¼ˆ2ï¼‰**æƒé‡è¡°å‡** Weight Decay
åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥æ­£åˆ™åŒ–é¡¹ï¼ˆå¢åŠ å¯¹è¾ƒå¤§ç³»æ•°çš„æƒ©ç½šé¡¹ï¼‰ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚å¸¸ç”¨çš„æ­£åˆ™åŒ–æ–¹æ³•æœ‰ L1 æ­£åˆ™åŒ–å’Œ L2 æ­£åˆ™åŒ–ã€‚ä¾‹å¦‚ L2 æ­£åˆ™åŒ–ï¼š

$$
\begin{align*}
\tilde{L}(w)&=L(w) + \frac{1}{2}\lambda \|w\|^2 \\
w &\leftarrow w - \eta \left(\frac{\partial L}{\partial w} + \lambda w\right)
\end{align*}
$$

ï¼ˆ3ï¼‰**AdaGrad**: Adaptive Gradient

åˆ©ç”¨æ¢¯åº¦å¹³æ–¹ç´¯åŠ å’Œçš„å¹³æ–¹æ ¹ã€‚ä¸åŒçš„å‚æ•°ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ï¼š

$$
\begin{align*}
c_t &= \sum_{j=1}^{t} \left( \nabla_\theta L(\theta_j) \right)^2 \\
\theta_{t+1} &= \theta_t - \eta \frac{\nabla_\theta L(\theta_t)}{\sqrt{c_t} + \varepsilon}
\end{align*}
$$

ä¸åŒå‚æ•°çš„å­¦ä¹ ç‡ä¾èµ–äº $c_t$ . å¯¹ä½é¢‘å‚æ•°åšè¾ƒå¤§çš„æ›´æ–°ï¼Œå¯¹é«˜é¢‘å‚æ•°åšè¾ƒå°çš„æ›´æ–°ï¼Œæå‡äº† SGD çš„é²æ£’æ€§ï¼Œå¯¹äºç¨€ç–æ•°æ®è¡¨ç°å¥½ã€‚

ï¼ˆ4ï¼‰**RMSprop**: Root-Mean-Square Prop

ä¸ AdaGrad æ€æƒ³ç±»ä¼¼ï¼Œåˆ©ç”¨æ¢¯åº¦å¹³æ–¹åŠ æƒå–å€’æ•°å¯¹æ¢¯åº¦è¿›è¡ŒåŠ æƒã€‚å®šä¹‰ RMS æ¢¯åº¦ï¼š

$$
\begin{align*}
s_t &= \gamma s_{t-1} + (1 - \gamma) \left( \nabla_\theta L(\theta_t) \right)^2 \\
\theta_{t+1} &= \theta_t - \eta \frac{\nabla_\theta L(\theta_t)}{\sqrt{s_t} + \varepsilon}
\end{align*}
$$

ä¿è¯å„ç»´åº¦å¯¼æ•°åœ¨ä¸€ä¸ªé‡çº§ï¼Œå‡å°‘æ‘†åŠ¨ã€‚[Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) å»ºè®® $\gamma=0.9,\mu=0.001$ .

ï¼ˆ5ï¼‰**Adam**: Adaptive Moment Optimization

Adam æ˜¯ RMSprop å’Œ Momentum çš„ç»“åˆã€‚è®¡ç®—æ¢¯åº¦å’Œæ¢¯åº¦å¹³æ–¹çš„å¹³æ»‘å¹³å‡ï¼Œåˆ©ç”¨æ¢¯åº¦çš„ä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©çš„æŒ‡æ•°åŠ æƒå¹³å‡ã€‚è¿™ä¹Ÿæ˜¯ç›®å‰ä¸»æµçš„ä¼˜åŒ–å™¨ã€‚

$$
\begin{align*}
v_t &= \beta_1 v_{t-1} + (1 - \beta_1) \nabla_\theta L(\theta_t) \\
s_t &= \beta_2 s_{t-1} + (1 - \beta_2) \left( \nabla_\theta L(\theta_t) \right)^2 \\
\theta_{t+1} &= \theta_t - \eta \frac{v_t}{\sqrt{s_t} + \varepsilon}
\end{align*}
$$

å‚æ•°è®¾ç½®ï¼š $\beta_1 = 0.9$ ï¼Œ $\beta_2$ æ¥è¿‘äº $1$ ï¼Œä¾‹å¦‚ $0.9999$ .

---

å‚æ•°åˆå§‹åŒ–æ–¹æ³•

ä¸€èˆ¬ åç½® åˆå§‹åŒ–ä¸º0ï¼Œè€Œ æƒé‡ çš„åˆå§‹åŒ–æ–¹æ³•æœ‰ï¼š

éšæœºåˆå§‹åŒ–ï¼šæƒé‡åˆå§‹åŒ–ä¸º0ï¼Œæ ‡å‡†å·®ä¸º $\sigma$ ï¼Œåˆ™ $w_i^l \sim \mathcal{N}(0, \sigma^2)$

Xavier åˆå§‹åŒ–ï¼š

$$
w_i^l \sim \mathcal{N} \left( 0, \frac{2}{n_l + n_{l-1}} \right) ,\quad w_i^l \sim \mathcal{U} \left[ -\sqrt{\frac{6}{n_l + n_{l-1}}}, \sqrt{\frac{6}{n_l + n_{l-1}}} \right]
$$

Kaiming åˆå§‹åŒ–ï¼š

$$
w_i^l \sim \mathcal{N} \left( 0, \frac{2}{n_l} \right) ,\quad w_i^l \sim \mathcal{U} \left[ -\sqrt{\frac{6}{n_l}}, \sqrt{\frac{6}{n_l}} \right]
$$

---

åŠ¨æ€å­¦ä¹ ç‡ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®è¿­ä»£æ¬¡æ•°é€æ­¥è°ƒæ•´å­¦ä¹ ç‡ã€‚åœ¨è®­ç»ƒåˆæœŸï¼Œç¦»ç›®æ ‡æŸå¤±è¿œï¼Œè®¾ç½®è¾ƒå¤§å­¦ä¹ ç‡ã€‚è®­ç»ƒä¸€æ®µæ—¶é—´åï¼Œå·²åˆ°è¾¾ç›®æ ‡æŸå¤±é™„è¿‘ï¼Œè·ç¦»æœ€ä¼˜ç‚¹è¿‘ï¼Œé€‰æ‹©è¾ƒå°å­¦ä¹ ç‡ã€‚

---

æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

ä»¥ Sigmoid å‡½æ•°ä¸ºä¾‹ï¼š

$$
\begin{gather*}
\sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \sigma(z)(1 - \sigma(z)) \in \left(0, \frac{1}{4}\right] \\
\frac{\partial L}{\partial b_1}\leqslant \left(\frac{1}{4}\right)^n w_2 w_3 \cdots w_n \frac{\partial L}{\partial b_n}
\end{gather*}
$$

éšç€ç½‘ç»œæ·±åº¦çš„åŠ æ·±ï¼Œå¹‚æŒ‡æ•°é¡¹å¾ˆå¿«è¶‹äº0ï¼Œæ¢¯åº¦è¡°å‡éå¸¸ä¸¥é‡ï¼Œæ¢¯åº¦æ¶ˆå¤±å¯¼è‡´æ— æ³•ç»§ç»­è®­ç»ƒã€‚

å¦‚ä½•ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼š

1. åˆ†å±‚é¢„è®­ç»ƒï¼šHinton äº2006å¹´æå‡ºï¼Œåˆ©ç”¨æ— ç›‘ç£æ•°æ®è¿›è¡Œåˆ†å±‚é¢„è®­ç»ƒï¼Œå†åˆ©ç”¨æœ‰ç›‘ç£æ•°æ®è°ƒæ•´ç½‘ç»œå‚æ•°ã€‚
2. ReLU æ¿€æ´»å‡½æ•°ã€‚ç¼“è§£ Sigmoid å’Œ tanh æ¿€æ´»å‡½æ•°å­˜åœ¨è¾ƒä¸¥é‡çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
3. Batch Normalizationï¼šé€å±‚å¯¹æ•°æ®è¿›è¡Œå°ºåº¦å½’ä¸€åŒ–ã€‚
4. è¾…åŠ©æŸå¤±å‡½æ•°ï¼šå¯¹æµ…å±‚ç¥ç»å…ƒè¾“å‡ºå»ºç«‹è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œç›´æ¥ä¼ é€’æ¢¯åº¦ã€‚åœ¨ GoogleNet ä¸­ä½¿ç”¨åˆ°ã€‚

---

æ¢¯åº¦çˆ†ç‚¸é—®é¢˜

$$
\frac{\partial L}{\partial b_1} = \sigma'(b_1) w_2 \sigma'(b_2) w_3 \cdots \sigma'(b_{N-1}) w_N \frac{\partial L}{\partial b_N},\quad
\text{if }  \left| w_j \sigma'(b_j)\right| > 1 \text{ then }\frac{\partial L}{\partial b_1} \gg 1
$$

å¦‚ä½•ç¼“è§£æ¢¯åº¦çˆ†ç‚¸ï¼š

1. é‡æ–°è®¾è®¡ç½‘ç»œæ¨¡å‹/æ›´æ¢æ¿€æ´»å‡½æ•°ã€‚
2. RNN ä¸­ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°å¯å‡å°‘æ¢¯åº¦çˆ†ç‚¸ã€‚
3. ä½¿ç”¨æ¢¯åº¦æˆªæ–­(gradient clipping)ã€‚
4. æƒé‡æ­£åˆ™åŒ–ï¼Œå¢åŠ  L1/L2 æ­£åˆ™æƒ©ç½šé¡¹ã€‚
